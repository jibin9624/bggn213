---
title: "class09"
author: 'Jibin (PID: A53300326)'
date: "2021/10/27"
output: html_document
---

# *Exploratory data analysis*

### Use the read.csv() function to read the CSV (comma-separated values) file containing the data (available from our class website: WisconsinCancer.csv )

```{r}
fna.data <-read.csv("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv")
```


### Complete the following code to input the data and store as wisc.df
```{r}
wisc.df <- data.frame (fna.data, row.names=1)
```

### We can use -1 here to remove the first column
```{r}
wisc.data <- wisc.df[,-1]
```

### Create diagnosis vector for later 
```{r}
diagnosis <- factor(wisc.df$diagnosis) 
diagnosis
```

> Q1. How many observations are in this dataset?

```{r}
dim (wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
grep("_mean", colnames(wisc.data))
```

# *Principal Component Analysis*

## Performing PCA

### Check column means and standard deviations

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

### Perform PCA on wisc.data by completing the following code

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

### 44.27%


> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

### 3 PCs are required to describe at least 70% of the original variance in the data.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

### 7 PCs are to describe at least 90% of the original variance in the data.


## Interpreting PCA results

### Create a biplot of the wisc.pr using the biplot() function.

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

## This is a hot mess of a plot and it's diffcult to interpret.


## Scatter plot observations by components 1 and 2

```{r}
plot(wisc.pr$x [,1:2],  xlab = "PC1", ylab = "PC2", col=diagnosis)
plot(wisc.pr$x [,1], wisc.pr$x [,3])
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

### Repeat for components 1 and 3
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

## The first plot (PC2 vs PC1) has a cleaner cut separating the two subgroups than this polt, since PC 2 explains more variance in the original data than PC 3.

### Create a data.frame for ggplot

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

### Make a scatter plot colored by diagnosis
```{r}
library("ggplot2")
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point() 
```

## Variance explained

### Calculate variance of each component

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

### Variance explained by each principal component: pve
```{r}
pve <- pr.var / sum(pr.var)
pve
```


### Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

### Alternative scree plot of the same data, note data driven y-axis
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

### ggplot based graph
#install.packages("factoextra")
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

# Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
barplot(wisc.pr$rotation[,1], las=2, mar = rep(10,4))
```
### concave.points_mean

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data

```{r}
var <- summary(wisc.pr)
var$importance
sum(var$importance[3,] <0.8)
```
### So, four at least.

## Hierarchical clustering

### Scale the wisc.data data using the "scale()" function???
```{r}
data.scaled <-scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist)
```

Results of hierarchical clustering

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(wisc.hclust, h=18, col="red", lty=2)
```

## Selecting number of clusters

### Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

Perhaps no.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 5)
table(wisc.hclust.clusters, diagnosis)
```


> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

### The method "ward.D2" looked much better, as he two major clusters are more clearly separated in this method

```{r}
plot(hclust(data.dist, method = "average"))
plot(hclust(data.dist, method = "complete"))
plot(hclust(data.dist, method = "single"))
plot(hclust(data.dist, method = "ward.D2"))
```

# OPTIONAL: K-means clustering

## K-means clustering and comparing results

```{r}
wisc.km <- kmeans(wisc.data, centers= 2, nstart= 20)

table(wisc.km$cluster, diagnosis )
table(wisc.km$cluster, wisc.hclust.clusters)
```

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

## It works well, but less precise that hclust.

# Combining methods

## Clustering on PCA results

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
```

